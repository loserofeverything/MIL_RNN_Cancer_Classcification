{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amino.acid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>-0.591</td>\n",
       "      <td>-1.302</td>\n",
       "      <td>-0.733</td>\n",
       "      <td>1.570</td>\n",
       "      <td>-0.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>-1.343</td>\n",
       "      <td>0.465</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-1.020</td>\n",
       "      <td>-0.255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>1.050</td>\n",
       "      <td>0.302</td>\n",
       "      <td>-3.656</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-3.242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>1.357</td>\n",
       "      <td>-1.453</td>\n",
       "      <td>1.477</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>-1.006</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>1.891</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>0.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G</th>\n",
       "      <td>-0.384</td>\n",
       "      <td>1.652</td>\n",
       "      <td>1.330</td>\n",
       "      <td>1.045</td>\n",
       "      <td>2.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H</th>\n",
       "      <td>0.336</td>\n",
       "      <td>-0.417</td>\n",
       "      <td>-1.673</td>\n",
       "      <td>-1.474</td>\n",
       "      <td>-0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>-1.239</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>2.131</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>1.831</td>\n",
       "      <td>-0.561</td>\n",
       "      <td>0.533</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>1.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L</th>\n",
       "      <td>-1.019</td>\n",
       "      <td>-0.987</td>\n",
       "      <td>-1.505</td>\n",
       "      <td>1.266</td>\n",
       "      <td>-0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>-0.663</td>\n",
       "      <td>-1.524</td>\n",
       "      <td>2.219</td>\n",
       "      <td>-1.005</td>\n",
       "      <td>1.212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>0.945</td>\n",
       "      <td>0.828</td>\n",
       "      <td>1.299</td>\n",
       "      <td>-0.169</td>\n",
       "      <td>0.933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.189</td>\n",
       "      <td>2.081</td>\n",
       "      <td>-1.628</td>\n",
       "      <td>0.421</td>\n",
       "      <td>-1.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0.931</td>\n",
       "      <td>-0.179</td>\n",
       "      <td>-3.005</td>\n",
       "      <td>-0.503</td>\n",
       "      <td>-1.853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>1.538</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>1.502</td>\n",
       "      <td>0.440</td>\n",
       "      <td>2.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>-0.228</td>\n",
       "      <td>1.399</td>\n",
       "      <td>-4.760</td>\n",
       "      <td>0.670</td>\n",
       "      <td>-2.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T</th>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.326</td>\n",
       "      <td>2.213</td>\n",
       "      <td>0.908</td>\n",
       "      <td>1.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V</th>\n",
       "      <td>-1.337</td>\n",
       "      <td>-0.279</td>\n",
       "      <td>-0.544</td>\n",
       "      <td>1.242</td>\n",
       "      <td>-1.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>-0.595</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.672</td>\n",
       "      <td>-2.128</td>\n",
       "      <td>-0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <td>0.260</td>\n",
       "      <td>0.830</td>\n",
       "      <td>3.097</td>\n",
       "      <td>-0.838</td>\n",
       "      <td>1.512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f1     f2     f3     f4     f5\n",
       "amino.acid                                   \n",
       "A          -0.591 -1.302 -0.733  1.570 -0.146\n",
       "C          -1.343  0.465 -0.862 -1.020 -0.255\n",
       "D           1.050  0.302 -3.656 -0.259 -3.242\n",
       "E           1.357 -1.453  1.477  0.113 -0.837\n",
       "F          -1.006 -0.590  1.891 -0.397  0.412\n",
       "G          -0.384  1.652  1.330  1.045  2.064\n",
       "H           0.336 -0.417 -1.673 -1.474 -0.078\n",
       "I          -1.239 -0.547  2.131  0.393  0.816\n",
       "K           1.831 -0.561  0.533 -0.277  1.648\n",
       "L          -1.019 -0.987 -1.505  1.266 -0.912\n",
       "M          -0.663 -1.524  2.219 -1.005  1.212\n",
       "N           0.945  0.828  1.299 -0.169  0.933\n",
       "P           0.189  2.081 -1.628  0.421 -1.392\n",
       "Q           0.931 -0.179 -3.005 -0.503 -1.853\n",
       "R           1.538 -0.055  1.502  0.440  2.897\n",
       "S          -0.228  1.399 -4.760  0.670 -2.647\n",
       "T          -0.032  0.326  2.213  0.908  1.313\n",
       "V          -1.337 -0.279 -0.544  1.242 -1.262\n",
       "W          -0.595  0.009  0.672 -2.128 -0.184\n",
       "Y           0.260  0.830  3.097 -0.838  1.512"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义数据\n",
    "data = \"A,-0.591,-1.302,-0.733,1.570,-0.146;C,-1.343,0.465,-0.862,-1.020,-0.255;D,1.050,0.302,-3.656,-0.259,-3.242;E,1.357,-1.453,1.477,0.113,-0.837;F,-1.006,-0.590,1.891,-0.397,0.412;G,-0.384,1.652,1.330,1.045,2.064;H,0.336,-0.417,-1.673,-1.474,-0.078;I,-1.239,-0.547,2.131,0.393,0.816;K,1.831,-0.561,0.533,-0.277,1.648;L,-1.019,-0.987,-1.505,1.266,-0.912;M,-0.663,-1.524,2.219,-1.005,1.212;N,0.945,0.828,1.299,-0.169,0.933;P,0.189,2.081,-1.628,0.421,-1.392;Q,0.931,-0.179,-3.005,-0.503,-1.853;R,1.538,-0.055,1.502,0.440,2.897;S,-0.228,1.399,-4.760,0.670,-2.647;T,-0.032,0.326,2.213,0.908,1.313;V,-1.337,-0.279,-0.544,1.242,-1.262;W,-0.595,0.009,0.672,-2.128,-0.184;Y,0.260,0.830,3.097,-0.838,1.512\"\n",
    "\n",
    "# 将数据分割成列表\n",
    "data_list = [item.split(',') for item in data.split(';')]\n",
    "\n",
    "# 创建DataFrame\n",
    "atchley = pd.DataFrame(data_list, columns=[\"amino.acid\", \"f1\", \"f2\", \"f3\", \"f4\", \"f5\"])\n",
    "\n",
    "# 将f1-f5列转换为数值类型\n",
    "atchley[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"]] = atchley[[\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"]].apply(pd.to_numeric)\n",
    "\n",
    "# 将amino.acid列设置为索引\n",
    "atchley.set_index(\"amino.acid\", inplace=True)\n",
    "atchley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atchley_dict = {row[0]: list(row[1:]) for row in atchley.itertuples()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/xiongjun/test/MIL/share/MixResult_UID_All/H2001H023.clonotypes.TRB.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_type = 'lung'\n",
    "missingfile = \"H2001H023.clonotypes.TRB.txt\"\n",
    "directory = \"/xiongjun/test/MIL/share/MixResult_UID_All\"\n",
    "file_path = os.path.join(directory, missingfile)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['breast', 'prostate', 'lung', 'liver', 'pancreas', 'colorectal', 'health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，将氨基酸序列转换为数值列表\n",
    "def sequence_to_values(sequence):\n",
    "    return [atchley_dict[amino_acid] for amino_acid in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calRA(raw_data, sample_name, keep, types, RA_save_dir ,amino_acids=set('ACDEFGHIKLMNPQRSTVWY')):\n",
    "    if len(raw_data) <= 1000:\n",
    "        return\n",
    "    raw_data['aaSeqCDR3_length'] = raw_data['aaSeqCDR3'].str.len()\n",
    "\n",
    "    # 计算所有aaSeqCDR3长度相同的行中的cloneCount的和\n",
    "    grouped = raw_data.groupby('aaSeqCDR3_length')['cloneFraction'].sum()\n",
    "\n",
    "    # 对grouped进行排序\n",
    "    sorted_grouped = grouped.sort_values(ascending=False)\n",
    "\n",
    "    # 计算累积和\n",
    "    cumsum = sorted_grouped.cumsum()\n",
    "\n",
    "    # 找到保持原来cloneCount总数100*keep%以上的那些行\n",
    "    mask = cumsum <= cumsum.iloc[-1] * keep\n",
    "    if not mask.any():  # 如果 mask 全为 False\n",
    "        mask.iloc[0] = True  # 将第一行的值设为 True\n",
    "    filtered_grouped = sorted_grouped[mask]\n",
    "    data_filtered = raw_data[raw_data['aaSeqCDR3_length'].isin(filtered_grouped.index)]\n",
    "    num_rows_filtered = len(data_filtered)\n",
    "    max_length = data_filtered['aaSeqCDR3_length'].max()\n",
    "    del raw_data\n",
    "    # 去掉第一个和最后三个值\n",
    "    data_filtered['aaSeqCDR3'] = data_filtered['aaSeqCDR3'].apply(lambda x: x[1:-3])\n",
    "    # 找到aaSeqCDR3中元素的最大长度\n",
    "    max_length = data_filtered['aaSeqCDR3'].str.len().max()\n",
    "    # 使用'-'在末尾进行填充到aaSeqCDR3中元素的最大长度\n",
    "    data_filtered['aaSeqCDR3'] = data_filtered['aaSeqCDR3'].apply(lambda x: x.ljust(max_length, '-'))\n",
    "    length = len(data_filtered['aaSeqCDR3'].iloc[0])\n",
    "\n",
    "    # 对于每个4-mer序列\n",
    "    for i in range(length - 3):\n",
    "        # 创建新的列\n",
    "        data_filtered[f'4-mer-{i+1}'] = data_filtered['aaSeqCDR3'].apply(lambda x: x[i:i+4] if set(x[i:i+4]).issubset(amino_acids) else np.nan)\n",
    "\n",
    "    \n",
    "    df_backup = data_filtered.copy()\n",
    "\n",
    "    # 找到所有的 '4-mer' 列\n",
    "    four_mer_columns = data_filtered.filter(regex='4-mer')\n",
    "\n",
    "    # 将 '4-mer' 列和 'ratio' 列合并\n",
    "    df_melted = pd.melt(data_filtered, id_vars='cloneFraction', value_vars=four_mer_columns.columns, var_name='4-mer_col', value_name='4-mer')\n",
    "    # 删除4-mer列中的NaN值\n",
    "    df_melted = df_melted.dropna(subset=['4-mer'])\n",
    "    # 计算每种4-mer的最大ratio值\n",
    "    TCR_RA_stats = df_melted.groupby('4-mer')['cloneFraction'].max().reset_index()\n",
    "    TCR_RA_stats.columns = ['4-mer', 'RA']\n",
    "\n",
    "\n",
    "    data_filtered = df_backup\n",
    "    # 初始化一个字典来存储每种4-mer序列的相对丰度值\n",
    "    relative_abundance = {}\n",
    "\n",
    "    # 遍历每一行数据\n",
    "    for index, row in data_filtered.iterrows():\n",
    "        # 遍历除了最后一列（cloneFraction）以外的所有列\n",
    "        for col in four_mer_columns.columns:\n",
    "            # 如果该列的值不是NaN，则将其相对丰度值累加到相应的键中\n",
    "            if not pd.isna(row[col]):\n",
    "                # 使用setdefault方法来初始化字典中键的默认值为0，然后累加cloneFraction值\n",
    "                relative_abundance.setdefault(row[col], 0)\n",
    "                relative_abundance[row[col]] += row['cloneFraction']\n",
    "\n",
    "\n",
    "\n",
    "    _4mer_RA_stats = pd.DataFrame(list(relative_abundance.items()), columns=['4-mer', 'RA'])\n",
    "    sumra = _4mer_RA_stats['RA'].sum()\n",
    "    _4mer_RA_stats['RA'] = _4mer_RA_stats['RA'] / sumra\n",
    "    RA_stats = _4mer_RA_stats.merge(TCR_RA_stats, on='4-mer', suffixes=('_4mer', '_TCR'))\n",
    "\n",
    "    # 使用 '4-mer' 列的值创建新的列\n",
    "    RA_stats['data'] = RA_stats['4-mer'].apply(sequence_to_values)\n",
    "    RA_stats['sample'] = sample_name\n",
    "    label = values.index(types)\n",
    "    RA_stats['label'] = label\n",
    "    RA_stats.set_index('4-mer', inplace=True)\n",
    "    return RA_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRA(filename, keep):\n",
    "    # 读取文件\n",
    "    path_parts = os.path.split(filename)\n",
    "    name = path_parts[1]\n",
    "    raw_data = pd.read_csv(filename, sep='\\t')\n",
    "    raw_data['sample'] = name.split('.')[0] \n",
    "    types = cancer_type\n",
    "    # 创建一个新的列来存储aaSeqCDR3列中元素的长度\n",
    "    df_group = raw_data.groupby('sample')\n",
    "    RA_save = str(keep) + '_RA'\n",
    "    for  sample_id, (sample_name, df) in enumerate(df_group):\n",
    "        RA = calRA(df, sample_name, keep, types, RA_save)\n",
    "    return RA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_72416/1651696834.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['aaSeqCDR3'] = data_filtered['aaSeqCDR3'].apply(lambda x: x[1:-3])\n",
      "/tmp/ipykernel_72416/1651696834.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered['aaSeqCDR3'] = data_filtered['aaSeqCDR3'].apply(lambda x: x.ljust(max_length, '-'))\n",
      "/tmp/ipykernel_72416/1651696834.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_filtered[f'4-mer-{i+1}'] = data_filtered['aaSeqCDR3'].apply(lambda x: x[i:i+4] if set(x[i:i+4]).issubset(amino_acids) else np.nan)\n"
     ]
    }
   ],
   "source": [
    "RA = readRA(file_path, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RA.to_csv(\"/xiongjun/test/MIL/testsinglefile/0/RA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from utils import  MyLoss, Preparation, test_RNN\n",
    "from models import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Set cuDNN configurations for determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "train_loss = []\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "\n",
    "topk = 40\n",
    "best_acc = 0\n",
    "best_model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"4mer\"\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "batch_size2 = 1\n",
    "# rnn = nn.RNN(input_size=128, hidden_size=256, num_layers=2, batch_first=True).to(device)\n",
    "rnn = RNN(128, 256, 2, 7, device)\n",
    "criterion = MyLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), 0.1, momentum=0.9, dampening=0, weight_decay=1e-4, nesterov=True)\n",
    "RA_path = \"/xiongjun/test/MIL/testsinglefile/*/*.csv\"\n",
    "MIL_dir = \"40_128_4mer\"\n",
    "test_save_dir = \"/xiongjun/test/MIL/testsinglefile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rnndataset, read_files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class siglefiletest(Preparation):\n",
    "    def loadData(self):\n",
    "        \"\"\"\n",
    "        加载数据集。\n",
    "\n",
    "        Returns:\n",
    "            tuple: 训练集、验证集和测试集的数据。\n",
    "\n",
    "        \"\"\"\n",
    "        ra_src = self.RA_path.split('/')[-3]\n",
    "        s = \"[\" + \"-\".join(map(str, self.ratio)) + \"]\"\n",
    "        unique_dataset_name = ra_src + \"_\" + s\n",
    "        path = os.path.join(self.save_dir, self.dataset_dir, unique_dataset_name)\n",
    "        \n",
    "        if os.path.exists(path) is False:\n",
    "            os.mkdir(path)\n",
    "        datasets = []\n",
    "        file_names = ['train.pkl', 'val.pkl', 'test.pkl']\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(path, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                    datasets.append(data)\n",
    "            else:\n",
    "                break\n",
    "        if len(datasets) == 3:\n",
    "            return datasets[0], datasets[1], datasets[2]\n",
    "        \n",
    "        train_files = []\n",
    "        val_files = []\n",
    "        test_files = []\n",
    "        RA_files = glob.glob(self.RA_path)\n",
    "        labels = set([file.split('/')[-2] for file in RA_files])\n",
    "        # 对每个标签，随机分配文件到训练集、验证集和测试集\n",
    "        for label in labels:\n",
    "            label_files = [file for file in RA_files if file.split('/')[-2] == label]\n",
    "            np.random.shuffle(label_files)\n",
    "            n = len(label_files)\n",
    "            train_files += label_files[:int(n*self.ratio[0])]\n",
    "            val_files += label_files[int(n*self.ratio[0]):int(n*(self.ratio[1] + self.ratio[0]))]\n",
    "            test_files += label_files[int(n*(1.0-self.ratio[2])):]\n",
    "\n",
    "        print(f\"Train: {len(train_files)} Val: {len(val_files)} Test: {len(test_files)}\")\n",
    "        train_dataset = rnndataset(read_files(train_files))\n",
    "        with open(os.path.join(path, 'train.pkl'), 'wb') as f:\n",
    "            pickle.dump(train_dataset, f)\n",
    "        return train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare = siglefiletest(test_save_dir, MIL_dir, RA_path,\\\n",
    "        device = device,ratio = [1.0, 0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1 Val: 0 Test: 0\n",
      "df rows:  64626\n"
     ]
    }
   ],
   "source": [
    "# load or make dataset\n",
    "train_dataset = prepare.loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare.loadModel(20, 128, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.emb\n",
    "train_dataloader = prepare.extractDataLoader(train_dataset, model, 1, 40, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.load_state_dict(torch.load(\"/xiongjun/test/MIL/testsinglefile/RNN/40_128_4mer_256_2/RNN-0-1.0.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating - Epoch: [114515] Batch: [1/1]\n",
      "Batch 1.0 Loss: 0.0013019903562963009 Acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "_loss_, _acc_ = test_RNN(114514, embs, rnn, train_dataloader, criterion, device, choice = \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013019903562963009 1.0\n"
     ]
    }
   ],
   "source": [
    "print(_loss_, _acc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
